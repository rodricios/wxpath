{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<pre>                            __  __  \n _      ___  ______  ____ _/ /_/ /_ \n| | /| / / |/_/ __ \\/ __ `/ __/ __ \\\n| |/ |/ /&gt;  &lt;/ /_/ / /_/ / /_/ / / /\n|__/|__/_/|_/ .___/\\__,_/\\__/_/ /_/ \n           /_/</pre>"},{"location":"#declarative-web-graph-traversal-with-xpath","title":"Declarative web graph traversal with XPath","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change. NEW: TUI - Interactive terminal interface (powered by Textual) for testing wxpath expressions and extracting data.</p> <p>wxpath is a declarative, deterministic (read: not powered by LLMs), async web crawler where traversal is expressed directly in XPath. Instead of writing imperative crawl loops, wxpath lets you describe what to follow and what to extract in a single expression.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import wxpath\n\nexpr = \"url('https://quotes.toscrape.com')//a/@href\"\n\nfor link in wxpath.wxpath_async_blocking_iter(expr):\n    print(link)\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Declarative Traversal - Express web crawling logic in XPath-like syntax</li> <li>RAG-Ready Output - Extract clean, structured JSON hierarchies directly from the graph</li> <li>Concurrent Execution - Async-first design with automatic concurrency management</li> <li>XPath 3.1 Support - Full XPath 3.1 features including maps and arrays via <code>elementpath</code></li> <li>Polite Crawling - Built-in robots.txt respect and adaptive throttling</li> <li>NEW: Persistent Crawls - Optional SQLite or Redis backends for persistent crawl results</li> <li>NEW: TUI - Interactive terminal interface for testing wxpath expressions</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install wxpath\n\n# Optional extras:\npip install wxpath[tui]           # Interactive Terminal UI\npip install wxpath[cache-sqlite]  # Persistence (SQLite)\npip install wxpath[cache-redis]   # Persistence (Redis)\n</code></pre>"},{"location":"#core-concepts","title":"Core Concepts","text":"<p>The wxpath DSL extends the familiar XPath syntax with additional operators for web traversal and data extraction.</p>"},{"location":"#the-url-operator","title":"The <code>url(...)</code> Operator","text":"<p>The <code>url(...)</code> operator fetches content from a URL and returns it as an <code>lxml.html.HtmlElement</code> for further XPath processing:</p> <pre><code># Fetch a page and extract all links\n\"url('https://example.com')//a/@href\"\n</code></pre>"},{"location":"#deep-crawling","title":"Deep Crawling","text":""},{"location":"#url","title":"<code>///url(...)</code>","text":"<p>The <code>///url(...)</code> syntax enables recursive crawling up to a specified <code>max_depth</code>:</p> <pre><code>path_expr = \"\"\"\nurl('https://quotes.toscrape.com')\n  ///url(//a/@href)\n    //a/@href\n\"\"\"\n\nfor item in wxpath.wxpath_async_blocking_iter(path_expr, max_depth=1):\n    print(item)\n</code></pre>"},{"location":"#url-follow","title":"<code>url('...', follow=...)</code>","text":"<p>The <code>follow</code> parameter allows you to specify a follow path for recursive crawling at the root node. While this may seem redundant and duplicated behavior found with the <code>///url</code> syntax, it is not. The <code>follow</code> parameter allows you to initiate, yes, a recursive crawl, however it also allows you to begin extracting data from the root node. This is useful in cases where you want to paginate through search pages.</p> <pre><code>path_expr = \"\"\"\nurl('https://quotes.toscrape.com/tag/humor/', follow=//li[@class='next']/a/@href)\n  //div[@class='quote']\n    /map{\n      'author': (./span/small/text())[1],\n      'text': (./span[@class='text']/text())[1]\n      }\n\"\"\"\n</code></pre>"},{"location":"#xpath-31-maps","title":"XPath 3.1 Maps","text":"<p>Extract structured data using XPath 3.1 map syntax:</p> <pre><code>path_expr = \"\"\"\nurl('https://example.com')\n  /map{\n    'title': //title/text() ! string(.),\n    'url': string(base-uri(.))\n  }\n\"\"\"\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started - Detailed setup and first crawl</li> <li>Language Design - Understanding wxpath expressions</li> <li>API Reference - Complete API documentation</li> <li>Examples - More usage examples</li> <li>NEW: TUI Quickstart - Interactive terminal interface</li> <li>NEW: RAG Integrations - Integrations with LangChain</li> </ul>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>Practical examples demonstrating wxpath usage patterns.</p>"},{"location":"examples/#basic-link-extraction","title":"Basic Link Extraction","text":"<p>Extract all links from a page:</p> <pre><code>import wxpath\n\nexpr = \"url('https://example.com')//a/@href\"\n\nfor link in wxpath.wxpath_async_blocking_iter(expr):\n    print(link)\n</code></pre>"},{"location":"examples/#structured-data-extraction","title":"Structured Data Extraction","text":"<p>Extract structured data using XPath 3.1 maps:</p> <pre><code>import wxpath\n\nexpr = \"\"\"\nurl('https://quotes.toscrape.com')\n  //div[@class='quote']/map{\n    'text': .//span[@class='text']/text(),\n    'author': .//small[@class='author']/text(),\n    'tags': .//a[@class='tag']/text()\n  }\n\"\"\"\n\nfor quote in wxpath.wxpath_async_blocking_iter(expr, max_depth=0):\n    print(quote)\n</code></pre>"},{"location":"examples/#paginated-crawl","title":"Paginated Crawl","text":"<p>Follow pagination links:</p> <pre><code>import wxpath\n\nexpr = \"\"\"\nurl('https://quotes.toscrape.com')\n  ///url(//li[@class='next']/a/@href)\n    //div[@class='quote']//span[@class='text']/text()\n\"\"\"\n\nfor quote in wxpath.wxpath_async_blocking_iter(expr, max_depth=10):\n    print(quote)\n</code></pre>"},{"location":"examples/#wikipedia-knowledge-graph","title":"Wikipedia Knowledge Graph","text":"<p>Build a knowledge graph from Wikipedia:</p> <pre><code>import wxpath\nfrom wxpath.settings import CRAWLER_SETTINGS\n\nCRAWLER_SETTINGS.headers = {\n    'User-Agent': 'my-app/1.0 (contact: you@example.com)'\n}\n\nexpr = \"\"\"\nurl('https://en.wikipedia.org/wiki/Python_(programming_language)')\n  ///url(\n    //div[@id='mw-content-text']//a/@href[\n      starts-with(., '/wiki/') and not(contains(., ':'))\n    ]\n  )\n  /map{\n    'title': (//h1/text())[1],\n    'url': string(base-uri(.)),\n    'summary': string((//div[contains(@class, 'mw-parser-output')]/p[1])[1]),\n    'links': //div[@id='mw-content-text']//a/@href[starts-with(., '/wiki/')]\n  }\n\"\"\"\n\nfor item in wxpath.wxpath_async_blocking_iter(expr, max_depth=1):\n    print(f\"{item['title']}: {item['url']}\")\n</code></pre>"},{"location":"examples/#custom-headers-and-engine","title":"Custom Headers and Engine","text":"<p>Configure crawling behavior:</p> <pre><code>from wxpath import wxpath_async_blocking_iter\nfrom wxpath.core.runtime import WXPathEngine\nfrom wxpath.http.client import Crawler\n\ncrawler = Crawler(\n    concurrency=4,\n    per_host=2,\n    timeout=15,\n    headers={\n        'User-Agent': 'research-bot/1.0 (academic research)',\n        'Accept-Language': 'en-US,en;q=0.9'\n    },\n    respect_robots=True\n)\n\nengine = WXPathEngine(crawler=crawler)\n\nexpr = \"url('https://example.com')///url(//a/@href)//title/text()\"\n\nfor title in wxpath_async_blocking_iter(expr, max_depth=2, engine=engine):\n    print(title)\n</code></pre>"},{"location":"examples/#async-usage","title":"Async Usage","text":"<p>Use with asyncio:</p> <pre><code>import asyncio\nfrom wxpath import wxpath_async\n\nasync def main():\n    expr = \"url('https://example.com')//a/@href\"\n\n    links = []\n    async for link in wxpath_async(expr, max_depth=1):\n        links.append(link)\n        if len(links) &gt;= 100:\n            break\n\n    return links\n\nlinks = asyncio.run(main())\n</code></pre>"},{"location":"examples/#progress-bar","title":"Progress Bar","text":"<p>Display crawl progress:</p> <pre><code>import wxpath\n\nexpr = \"\"\"\nurl('https://quotes.toscrape.com')\n  ///url(//li[@class='next']/a/@href)\n    //div[@class='quote']/map{\n      'text': .//span[@class='text']/text(),\n      'author': .//small[@class='author']/text()\n    }\n\"\"\"\n\nfor quote in wxpath.wxpath_async_blocking_iter(expr, max_depth=5, progress=True):\n    pass  # Progress bar shows crawl status\n</code></pre>"},{"location":"examples/#using-hooks","title":"Using Hooks","text":""},{"location":"examples/#filter-by-language","title":"Filter by Language","text":"<pre><code>from wxpath import hooks, wxpath_async_blocking_iter\n\n@hooks.register\nclass OnlyEnglish:\n    def post_parse(self, ctx, elem):\n        lang = elem.xpath('string(/html/@lang)').lower()[:2]\n        return elem if lang in (\"en\", \"\") else None\n\nfor item in wxpath_async_blocking_iter(expr, max_depth=1):\n    print(item)  # Only English pages\n</code></pre>"},{"location":"examples/#log-all-urls","title":"Log All URLs","text":"<pre><code>from wxpath import hooks\n\n@hooks.register\nclass URLLogger:\n    def post_fetch(self, ctx, html_bytes):\n        print(f\"Fetched: {ctx.url} (depth={ctx.depth})\")\n        return html_bytes\n</code></pre>"},{"location":"examples/#save-results-to-jsonl","title":"Save Results to JSONL","text":"<pre><code>from wxpath import hooks, wxpath_async_blocking_iter\n\nhooks.register(hooks.JSONLWriter(\"/output/results.jsonl\"))\n\nfor item in wxpath_async_blocking_iter(expr, max_depth=1):\n    pass  # Results automatically written to file\n</code></pre>"},{"location":"examples/#caching-for-development","title":"Caching for Development","text":"<p>Enable caching to speed up development iteration:</p> <pre><code>from wxpath.settings import CACHE_SETTINGS\nfrom wxpath import wxpath_async_blocking_iter\n\nCACHE_SETTINGS.enabled = True\n\n# First run: fetches from network\nresults1 = list(wxpath_async_blocking_iter(expr, max_depth=1))\n\n# Modify expression and run again - uses cached responses\nexpr2 = \"url('https://example.com')//h1/text()\"\nresults2 = list(wxpath_async_blocking_iter(expr2, max_depth=1))\n</code></pre>"},{"location":"examples/#error-handling","title":"Error Handling","text":"<p>Handle errors in crawl results:</p> <pre><code>from wxpath import wxpath_async_blocking_iter\n\nfor item in wxpath_async_blocking_iter(expr, max_depth=2, yield_errors=True):\n    if isinstance(item, dict) and item.get('__type__') == 'error':\n        print(f\"Error: {item['url']} - {item['reason']}\")\n        continue\n\n    process(item)\n</code></pre>"},{"location":"examples/#cli-examples","title":"CLI Examples","text":"<p>Basic extraction:</p> <pre><code>wxpath \"url('https://example.com')//a/@href\"\n</code></pre> <p>Deep crawl with filters:</p> <pre><code>wxpath --depth 2 \\\n    --header \"User-Agent: my-bot/1.0\" \\\n    \"url('https://example.com')///url(//a/@href[contains(., '/docs/')])//h1/text()\"\n</code></pre> <p>Save to file:</p> <pre><code>wxpath \"url('https://example.com')//a/@href\" &gt; links.jsonl\n</code></pre> <p>With caching:</p> <pre><code>wxpath --cache --depth 3 \"url('https://example.com')///url(//a/@href)//title/text()\"\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#getting-started","title":"Getting Started","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#basic-installation","title":"Basic Installation","text":"<pre><code>pip install wxpath\n</code></pre> <p>Requires Python 3.10+.</p>"},{"location":"getting-started/#optional-dependencies","title":"Optional Dependencies","text":"<p>For response caching:</p> <pre><code># SQLite backend (good for single-worker crawls)\npip install wxpath[cache-sqlite]\n\n# Redis backend (recommended for concurrent crawls)\npip install wxpath[cache-redis]\n</code></pre>"},{"location":"getting-started/#your-first-crawl","title":"Your First Crawl","text":""},{"location":"getting-started/#simple-link-extraction","title":"Simple Link Extraction","text":"<pre><code>import wxpath\n\n# Extract all links from a page\nexpr = \"url('https://quotes.toscrape.com')//a/@href\"\n\nfor link in wxpath.wxpath_async_blocking_iter(expr):\n    print(link)\n</code></pre>"},{"location":"getting-started/#using-async-api","title":"Using Async API","text":"<pre><code>import asyncio\nfrom wxpath import wxpath_async\n\nasync def main():\n    expr = \"url('https://quotes.toscrape.com')//a/@href\"\n    async for item in wxpath_async(expr, max_depth=1):\n        print(item)\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#understanding-wxpath-expressions","title":"Understanding wxpath Expressions","text":"<p>A wxpath expression consists of segments that describe:</p> <ol> <li>What to fetch - <code>url(...)</code> segments</li> <li>What to follow - <code>///url(...)</code> for recursive crawling</li> <li>What to extract - XPath expressions</li> </ol>"},{"location":"getting-started/#example-multi-level-crawl","title":"Example: Multi-level Crawl","text":"<pre><code>path_expr = \"\"\"\nurl('https://quotes.toscrape.com')\n  ///url(//a/@href)\n    //span[@class='text']/text()\n\"\"\"\n\nfor quote in wxpath.wxpath_async_blocking_iter(path_expr, max_depth=2):\n    print(quote)\n</code></pre>"},{"location":"getting-started/#example-structured-data-extraction","title":"Example: Structured Data Extraction","text":"<pre><code>from wxpath.settings import CRAWLER_SETTINGS\n\n# Custom headers for politeness; necessary for some sites (e.g., Wikipedia)\nCRAWLER_SETTINGS.headers = {'User-Agent': 'my-app/0.4.0 (contact: you@example.com)'}\n\npath_expr = \"\"\"\nurl('https://en.wikipedia.org/wiki/Python_(programming_language)')\n  /map{\n    'title': (//h1//text())[1] ! normalize-space(.),\n    'mainText': //div[contains(@class, 'mw-parser-output')]/string-join(//p) ! normalize-space(.),\n    'url': string(base-uri(.))\n  }\n\"\"\"\n\nfor item in wxpath.wxpath_async_blocking_iter(path_expr, max_depth=0):\n    print(item)\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":""},{"location":"getting-started/#custom-headers","title":"Custom Headers","text":"<p>Many websites require proper User-Agent headers:</p> <pre><code>from wxpath.settings import CRAWLER_SETTINGS\n\nCRAWLER_SETTINGS.headers = {\n    'User-Agent': 'my-crawler/1.0 (contact: you@example.com)'\n}\n</code></pre>"},{"location":"getting-started/#engine-configuration","title":"Engine Configuration","text":"<pre><code>from wxpath import wxpath_async_blocking_iter\nfrom wxpath.core.runtime import WXPathEngine\nfrom wxpath.http.client import Crawler\n\ncrawler = Crawler(\n    concurrency=8,\n    per_host=2,\n    timeout=10,\n    respect_robots=True,\n    headers={'User-Agent': 'my-app/0.1.0'}\n)\n\nengine = WXPathEngine(crawler=crawler)\n\nitems = list(wxpath_async_blocking_iter(\n    \"url('https://quotes.toscrape.com')//a/@href\",\n    max_depth=1,\n    engine=engine\n))\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Language Design - Deep dive into expression syntax</li> <li>Configuration - All configuration options</li> <li>Hooks - Extending wxpath with custom hooks</li> <li>CLI - Command-line interface usage</li> </ul>"},{"location":"api/","title":"Overview","text":""},{"location":"api/#api-reference","title":"API Reference","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p>"},{"location":"api/#top-level-api","title":"Top-Level API","text":"<p>The <code>wxpath</code> package exports these primary functions:</p> <pre><code>from wxpath import (\n    wxpath_async,\n    wxpath_async_blocking,\n    wxpath_async_blocking_iter,\n    configure_logging\n)\n</code></pre>"},{"location":"api/#wxpath_async","title":"wxpath_async","text":"<pre><code>async def wxpath_async(\n    path_expr: str,\n    max_depth: int,\n    progress: bool = False,\n    engine: WXPathEngine | None = None,\n    yield_errors: bool = False\n) -&gt; AsyncGenerator[Any, None]\n</code></pre> <p>Async generator that evaluates a wxpath expression and yields results.</p> <p>Parameters:</p> Name Type Description <code>path_expr</code> str wxpath expression to evaluate <code>max_depth</code> int Maximum crawl depth for <code>url</code> hops <code>progress</code> bool Display tqdm progress bar <code>engine</code> WXPathEngine Pre-configured engine instance <code>yield_errors</code> bool Yield error dicts instead of silently skipping <p>Yields: Extracted values (HtmlElement, WxStr, dict, etc.)</p>"},{"location":"api/#wxpath_async_blocking_iter","title":"wxpath_async_blocking_iter","text":"<pre><code>def wxpath_async_blocking_iter(\n    path_expr: str,\n    max_depth: int = 1,\n    progress: bool = False,\n    engine: WXPathEngine | None = None,\n    yield_errors: bool = False\n) -&gt; Iterator[Any]\n</code></pre> <p>Synchronous iterator wrapper around <code>wxpath_async</code>. Creates its own event loop.</p> <p>Warning: Must not be called from within an active asyncio event loop.</p> <p>Parameters: Same as <code>wxpath_async</code></p> <p>Yields: Extracted values</p>"},{"location":"api/#wxpath_async_blocking","title":"wxpath_async_blocking","text":"<pre><code>def wxpath_async_blocking(\n    path_expr: str,\n    max_depth: int = 1,\n    progress: bool = False,\n    engine: WXPathEngine | None = None,\n    yield_errors: bool = False\n) -&gt; list[Any]\n</code></pre> <p>Synchronous function that returns all results as a list.</p> <p>Parameters: Same as <code>wxpath_async</code></p> <p>Returns: List of all extracted values</p>"},{"location":"api/#configure_logging","title":"configure_logging","text":"<pre><code>def configure_logging(level: int = logging.INFO) -&gt; None\n</code></pre> <p>Configure wxpath's logging system.</p> <p>Parameters:</p> Name Type Default Description <code>level</code> (int str) logging.INFO"},{"location":"api/#module-index","title":"Module Index","text":""},{"location":"api/#core","title":"Core","text":"<ul> <li>Engine - Main execution engine (<code>WXPathEngine</code>)</li> <li>TODO: Parser - Expression parser and AST nodes</li> <li>TODO: Models - Data models (CrawlTask, intents)</li> <li>Operations - Operation handlers and registry</li> </ul>"},{"location":"api/#http","title":"HTTP","text":"<ul> <li> <p>Crawler - HTTP client (<code>Crawler</code>, <code>BaseCrawler</code>)</p> </li> <li> <p>TODO: Cache - Cache backend factory</p> </li> <li>TODO: Policy - Retry, robots, throttling policies</li> <li>TODO: Stats - Crawler statistics</li> </ul>"},{"location":"api/#hooks","title":"Hooks","text":"<ul> <li>TODO: Registry - Hook registration and protocol</li> <li>TODO: Built-in Hooks - Predefined hooks</li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":"<ul> <li>TODO: Logging - Logging configuration</li> <li>TODO: Serialize - Type simplification</li> </ul>"},{"location":"api/#configuration","title":"Configuration","text":"<ul> <li>Settings - Global settings (<code>SETTINGS</code>, <code>CRAWLER_SETTINGS</code>, <code>CACHE_SETTINGS</code>)</li> </ul>"},{"location":"api/settings/","title":"Settings","text":""},{"location":"api/settings/#settings","title":"Settings","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>Global configuration for wxpath.</p>"},{"location":"api/settings/#location","title":"Location","text":"<pre><code>from wxpath.settings import SETTINGS, CRAWLER_SETTINGS, CACHE_SETTINGS, AttrDict\n</code></pre>"},{"location":"api/settings/#settings_1","title":"SETTINGS","text":"<p>Root settings object containing all configuration. The SETTINGS dict hierarchy follows the hierarchy of wxpath submodules.</p> <pre><code>SETTINGS = AttrDict({\n    \"http\": {\n        \"client\": {\n            \"cache\": { ... },\n            \"crawler\": { ... }\n        }\n    }\n})\n</code></pre>"},{"location":"api/settings/#crawler_settings","title":"CRAWLER_SETTINGS","text":"<p>Shortcut to <code>SETTINGS.http.client.crawler</code>.</p> <pre><code>from wxpath.settings import CRAWLER_SETTINGS\n\nCRAWLER_SETTINGS.concurrency = 16\nCRAWLER_SETTINGS.headers = {\"User-Agent\": \"my-bot/1.0\"}\n</code></pre>"},{"location":"api/settings/#fields","title":"Fields","text":"Setting Type Default Description <code>concurrency</code> int 16 Global concurrent requests <code>per_host</code> int 8 Per-host concurrent requests <code>timeout</code> int 15 Request timeout in seconds <code>headers</code> dict <code>{...}</code> Default HTTP headers <code>proxies</code> dict <code>None</code> Per-host proxy mapping <code>respect_robots</code> bool <code>True</code> Honor robots.txt <code>auto_throttle_target_concurrency</code> float <code>None</code> Target concurrent requests for throttler <code>auto_throttle_start_delay</code> float 0.25 Initial throttle delay <code>auto_throttle_max_delay</code> float 10.0 Maximum throttle delay"},{"location":"api/settings/#cache_settings","title":"CACHE_SETTINGS","text":"<p>Shortcut to <code>SETTINGS.http.client.cache</code>.</p> <pre><code>from wxpath.settings import CACHE_SETTINGS\n\nCACHE_SETTINGS.enabled = True\nCACHE_SETTINGS.backend = \"redis\"\n</code></pre>"},{"location":"api/settings/#fields_1","title":"Fields","text":"Setting Type Default Description <code>enabled</code> bool <code>False</code> Enable response caching <code>expire_after</code> timedelta <code>timedelta(days=7)</code> Cache TTL in seconds <code>allowed_methods</code> tuple (\"GET\", \"HEAD\") HTTP methods to cache <code>allowed_codes</code> tuple (200, 203, 301, 302, 307, 308) Status codes to cache <code>ignored_params</code> list [\"utm_*\", \"fbclid\"] Query params to ignore in cache key <code>backend</code> str \"sqlite\" Cache backend (\"sqlite\" or \"redis\") <code>sqlite</code> dict <code>{...}</code> SQLite backend settings <code>redis</code> dict <code>{...}</code> Redis backend settings"},{"location":"api/settings/#sqlite-settings","title":"SQLite Settings","text":"Setting Type Default Description <code>cache_name</code> str \"cache.db\" SQLite cache name"},{"location":"api/settings/#redis-settings","title":"Redis Settings","text":"Setting Type Default Description <code>redis.address</code> str \"redis://localhost:6379/0\" Redis connection URL <code>cache_name</code> str \"wxpath:\" Redis cache name"},{"location":"api/settings/#attrdict","title":"AttrDict","text":"<p>Dictionary subclass enabling dot-notation access.</p> <pre><code>class AttrDict(dict):\n    def __getattr__(self, name): ...\n    def __setattr__(self, name, value): ...\n</code></pre>"},{"location":"api/settings/#usage","title":"Usage","text":"<pre><code>from wxpath.settings import AttrDict\n\nconfig = AttrDict({\n    \"database\": {\n        \"host\": \"localhost\",\n        \"port\": 5432\n    }\n})\n\n# Both work\nconfig[\"database\"][\"host\"]\nconfig.database.host\n\n# Setting values\nconfig.database.port = 5433\n</code></pre>"},{"location":"api/settings/#configuration-examples","title":"Configuration Examples","text":""},{"location":"api/settings/#basic-setup","title":"Basic Setup","text":"<pre><code>from wxpath.settings import CRAWLER_SETTINGS, CACHE_SETTINGS\n\n# Crawler config\nCRAWLER_SETTINGS.concurrency = 8\nCRAWLER_SETTINGS.per_host = 2\nCRAWLER_SETTINGS.headers = {\n    \"User-Agent\": \"my-crawler/1.0 (contact: me@example.com)\"\n}\n\n# Enable caching\nCACHE_SETTINGS.enabled = True\n</code></pre>"},{"location":"api/settings/#full-configuration","title":"Full Configuration","text":"<pre><code>from wxpath.settings import SETTINGS\n\n# Access nested settings\nSETTINGS.http.client.crawler.concurrency = 16\nSETTINGS.http.client.crawler.timeout = 60\n\nSETTINGS.http.client.cache.enabled = True\nSETTINGS.http.client.cache.backend = \"redis\"\nSETTINGS.http.client.cache.redis.address = \"redis://cache.local:6379/0\"\n</code></pre>"},{"location":"api/settings/#proxy-configuration","title":"Proxy Configuration","text":"<pre><code>from wxpath.settings import CRAWLER_SETTINGS\nfrom collections import defaultdict\n\n# Per-host proxies\nCRAWLER_SETTINGS.proxies = {\n    \"example.com\": \"http://proxy1:8080\",\n    \"api.example.com\": \"http://proxy2:8080\"\n}\n\n# Default proxy for all hosts\nCRAWLER_SETTINGS.proxies = defaultdict(lambda: \"http://default:8080\")\n</code></pre>"},{"location":"api/settings/#settings-precedence","title":"Settings Precedence","text":"<ol> <li>Constructor arguments (highest priority)</li> <li>Settings object modifications</li> <li>Default values (lowest priority)</li> </ol> <pre><code>from wxpath.http.client import Crawler\nfrom wxpath.settings import CRAWLER_SETTINGS\n\nCRAWLER_SETTINGS.concurrency = 8  # Setting\n\n# Constructor overrides settings\ncrawler = Crawler(concurrency=4)  # Uses 4, not 8\n</code></pre>"},{"location":"api/core/","title":"Overview","text":""},{"location":"api/core/#core-module","title":"Core Module","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>The <code>wxpath.core</code> module contains the expression parser, execution engine, and data models.</p>"},{"location":"api/core/#submodules","title":"Submodules","text":"Module Description engine Main execution engine (<code>WXPathEngine</code>) TODO: parser Pratt-style expression parser TODO: models Data models and intent types ops Operation handlers and registry"},{"location":"api/core/#quick-import","title":"Quick Import","text":"<pre><code>from wxpath.core.runtime import WXPathEngine\nfrom wxpath.core import parser\nfrom wxpath.core.models import CrawlTask, CrawlIntent, DataIntent\nfrom wxpath.core.ops import get_operator\n</code></pre>"},{"location":"api/core/#architecture","title":"Architecture","text":"<pre><code>Expression String\n            \u2502\n            \u25bc \n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Parser \u2502  \u2192 AST (Segments)\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n            \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Engine \u2502  \u2192 CrawlTasks \u2192  \u2502 Crawler \u2502 \n    \u250c\u2500\u2500 \u2502        \u2502  \u2190 HTML Body  \u2190  \u2502         \u2502\n    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502      \u2502  \u25b2  \n    \u2502      \u25bc  | Intents\n    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   \u2502   Ops   \u2502  \u2192 Executes segment operations\n    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502       \n    \u2502   \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2514\u2500\u25b6 \u2502 Results \u2502  \n        \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The parser converts expressions to an AST. The engine processes the AST with a BFS-like crawl queue. Operations handle each segment type and emit intents. Intents drive the next processing step (crawl, extract, or yield data).</p>"},{"location":"api/core/engine/","title":"Engine","text":""},{"location":"api/core/engine/#engine","title":"Engine","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>This model defines the execution engine, which coordinates crawling and extraction. </p> <p>Runtime flow:</p> <ol> <li>Parses wxpath expressions, retrieving an AST.</li> <li>Processes the AST by dispatching to operation handlers.</li> <li>Gather processing tasks internally known as \"intents\" from operation handlers. Depending on the intent, the engine may:<ol> <li>Crawl, (<code>CrawlIntent</code>) by communicating to the Crawler by enqueuing CrawlTasks.</li> <li>Extract, (<code>ExtractIntent</code>) by dispatching to operation handlers that extract data.</li> <li>Yield, (<code>DataIntent</code>) by yielding data to the caller.</li> <li>Process, (<code>ProcessIntent</code>) by dispatching to operation handlers, effectively bridging XPath and wxpath expressions.</li> <li>InfiniteCrawl, (<code>InfiniteCrawlIntent</code>) by enqueuing infinite crawl tasks, which communicates to the engine that it should produce crawl intents AND process intents or extract intents or data intents.</li> </ol> </li> </ol> <p>This engine evolved from an earlier, monolithic <code>wxpath</code> engine that had the crawler tightly coupled to the engine. While coupling between the crawler and engine still exists, it does so in a much more modular way, by dispatching crawl tasks to the crawler's queue.</p>"},{"location":"api/core/engine/#location","title":"Location","text":"<pre><code>from wxpath.core.runtime import WXPathEngine\nfrom wxpath.core.runtime.engine import HookedEngineBase\n</code></pre>"},{"location":"api/core/engine/#wxpathengine","title":"WXPathEngine","text":"<p>Main class for executing wxpath expressions.</p> <pre><code>class WXPathEngine(HookedEngineBase):\n    def __init__(\n        self,\n        crawler: Crawler | None = None,\n        concurrency: int = 16,\n        per_host: int = 8,\n        respect_robots: bool = True,\n        allowed_response_codes: set[int] = None,\n        allow_redirects: bool = True\n    )\n</code></pre>"},{"location":"api/core/engine/#parameters","title":"Parameters","text":"Name Type Default Description <code>crawler</code> Crawler None Pre-configured crawler instance <code>concurrency</code> int 16 Global concurrent fetches <code>per_host</code> int 8 Per-host concurrent fetches <code>respect_robots</code> bool True Honor robots.txt directives <code>allowed_response_codes</code> set[int] {200} Accepted HTTP status codes <code>allow_redirects</code> bool True Follow HTTP redirects"},{"location":"api/core/engine/#attributes","title":"Attributes","text":"Name Type Description <code>seen_urls</code> set[str] URLs already processed (deduplication) <code>crawler</code> Crawler HTTP crawler instance <code>allowed_response_codes</code> set[int] Accepted status codes <code>allow_redirects</code> bool Redirect following enabled"},{"location":"api/core/engine/#methods","title":"Methods","text":""},{"location":"api/core/engine/#run","title":"run","text":"<pre><code>async def run(\n    self,\n    expression: str,\n    max_depth: int,\n    progress: bool = False,\n    yield_errors: bool = False\n) -&gt; AsyncGenerator[Any, None]\n</code></pre> <p>Execute a wxpath expression concurrently and yield results.</p> <p>Parameters:</p> Name Type Description <code>expression</code> str wxpath expression to evaluate <code>max_depth</code> int Maximum crawl depth <code>progress</code> bool Show tqdm progress bar <code>yield_errors</code> bool Yield error dicts for failed requests <p>Yields: Extracted values (HtmlElement, WxStr, dict, etc.)</p>"},{"location":"api/core/engine/#example","title":"Example","text":"<pre><code>import asyncio\nfrom wxpath.core.runtime import WXPathEngine\nfrom wxpath.http.client import Crawler\n\ncrawler = Crawler(\n    concurrency=8,\n    per_host=2,\n    headers={'User-Agent': 'my-bot/1.0'}\n)\n\nengine = WXPathEngine(\n    crawler=crawler,\n    allowed_response_codes={200, 301, 302}\n)\n\nasync def main():\n    async for item in engine.run(\"url('https://example.com')//a/@href\", max_depth=1):\n        print(item)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/core/engine/#hookedenginebase","title":"HookedEngineBase","text":"<p>Base class providing hook invocation methods.</p> <pre><code>class HookedEngineBase:\n    async def post_fetch_hooks(\n        self, body: bytes | str, task: CrawlTask\n    ) -&gt; bytes | str | None\n\n    async def post_parse_hooks(\n        self, elem: HtmlElement | None, task: CrawlTask\n    ) -&gt; HtmlElement | None\n\n    async def post_extract_hooks(\n        self, value: Any\n    ) -&gt; Any | None\n</code></pre>"},{"location":"api/core/engine/#hook-methods","title":"Hook Methods","text":""},{"location":"api/core/engine/#post_fetch_hooks","title":"post_fetch_hooks","text":"<p>Run registered <code>post_fetch</code> hooks over a fetched response body.</p> <p>Parameters: - <code>body</code> - Raw response body bytes - <code>task</code> - The CrawlTask that produced the response</p> <p>Returns: Transformed body, or <code>None</code> if dropped</p>"},{"location":"api/core/engine/#post_parse_hooks","title":"post_parse_hooks","text":"<p>Run registered <code>post_parse</code> hooks on a parsed DOM element.</p> <p>Parameters: - <code>elem</code> - Parsed lxml element - <code>task</code> - The originating CrawlTask</p> <p>Returns: Transformed element, or <code>None</code> if dropped</p>"},{"location":"api/core/engine/#post_extract_hooks","title":"post_extract_hooks","text":"<p>Run registered <code>post_extract</code> hooks on extracted values.</p> <p>Parameters: - <code>value</code> - The extracted datum</p> <p>Returns: Transformed value, or <code>None</code> if dropped</p>"},{"location":"api/core/engine/#execution-flow","title":"Execution Flow","text":"<p>With the above Hooks system, wxpath's execution flow is as follows:</p> <ol> <li>Parse expression into segments</li> <li>Create seed task with initial URL(s)</li> <li>Process tasks via BFS queue:</li> <li>Fetch URL</li> <li>Run post_fetch hooks</li> <li>Parse HTML</li> <li>Run post_parse hooks</li> <li>Execute remaining segments</li> <li>Emit intents (crawl, extract, data)</li> <li>Yield extracted data through post_extract hooks</li> <li>Continue until queue exhausted or max_depth reached</li> </ol>"},{"location":"api/core/ops/","title":"Operations","text":""},{"location":"api/core/ops/#operations","title":"Operations","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>Operation handlers that execute wxpath segments. This module follows a dispatcher pattern, where each segment signature (wxpath function name or segment type, and its argument types) is mapped to a handler function.</p> <p>This module (along with the parser) can both be tightened up in the following ways: </p> <ol> <li>Better type checking.<ol> <li>Specifically, check that next segments are of the correct type.</li> </ol> </li> <li>Less intents (<code>ProcessIntent</code> may be unnecessary).</li> <li>More intuitive error messages.</li> </ol>"},{"location":"api/core/ops/#location","title":"Location","text":"<pre><code>from wxpath.core.ops import get_operator, OPS_REGISTER\n</code></pre>"},{"location":"api/core/ops/#get_operator","title":"get_operator","text":"<pre><code>def get_operator(binary_or_segment: Binary | Segment) -&gt; Callable\n</code></pre> <p>Retrieve the handler function for a AST node type.</p> <p>Parameters: - <code>binary_or_segment</code> - AST node to find handler for</p> <p>Returns: Handler function</p>"},{"location":"api/core/ops/#ops_register","title":"OPS_REGISTER","text":"<p>Global dictionary mapping segment signatures to handlers.</p> <pre><code>OPS_REGISTER: dict[tuple, Callable] = {}\n</code></pre>"},{"location":"api/core/ops/#handler-registration","title":"Handler Registration","text":"<p>Handlers are registered with the <code>@register</code> decorator:</p> <pre><code>from wxpath.core.ops import register\nfrom wxpath.core.parser import Xpath, String\n\n@register(Xpath)\ndef handle_xpath(elem, segments, depth):\n    # Execute XPath on element\n    ...\n    return [DataIntent(value=result)]\n\n@register('url', (String,))\ndef handle_url_literal(elem, segments, depth):\n    # Fetch literal URL\n    url = segments[0].args[0].value\n    return [CrawlIntent(url=url, next_segments=segments[1:])]\n</code></pre> <p>TODO: Converge on a common function parameter type for the register decorator. Right now it allows for AST node type OR string.</p>"},{"location":"api/core/ops/#registered-handlers","title":"Registered Handlers","text":""},{"location":"api/core/ops/#xpath-handler","title":"XPath Handler","text":"<p>Signature: <code>(Xpath,)</code></p> <p>Executes XPath expressions on elements.</p>"},{"location":"api/core/ops/#url-literal-handler","title":"URL Literal Handler","text":"<p>Signature: <code>('url', (String,))</code></p> <p>Yields a CrawlIntent for a literal URL. This signal eventually reaches the crawler.</p> <pre><code>\"url('https://example.com')\"\n</code></pre>"},{"location":"api/core/ops/#url-xpath-handler","title":"URL XPath Handler","text":"<p>Signature: <code>('url', (Xpath,))</code></p> <p>Yields CrawlIntents for URLs extracted by XPath.</p> <pre><code>\"url(//a/@href)\"\n</code></pre>"},{"location":"api/core/ops/#url-query-handler","title":"URL Query Handler","text":"<p>Signature: <code>('//url', ...)</code></p> <p>Yields CrawlIntents for URLs extracted by XPath.</p> <pre><code>\"//url(//a/@href)\"\n</code></pre>"},{"location":"api/core/ops/#url-crawl-handler","title":"URL Crawl Handler","text":"<p>Signature: <code>('///url', (Xpath,))</code></p> <p>Recursive deep crawling.</p> <pre><code>\"///url(//a/@href)\"\n</code></pre>"},{"location":"api/core/ops/#url-crawl-with-extraction-handler","title":"URL Crawl with Extraction Handler","text":"<p>Signature: <code>('///url', (Xpath, str))</code></p> <p>Deep crawl with inline extraction. Yields InfiniteCrawlIntent.</p>"},{"location":"api/core/ops/#binary-map-handler","title":"Binary (Map) Handler","text":"<p>Signature: <code>(Binary, ...)</code></p> <p>Handles the map operator (<code>!</code>). (More to come...)</p>"},{"location":"api/core/ops/#handler-return-values","title":"Handler Return Values","text":"<p>Handlers return a list of intents:</p> <pre><code>def my_handler(elem, segments, depth) -&gt; Iterable[Intent]:\n    return (\n        CrawlIntent(url=\"...\", next_segments=...),\n        DataIntent(value={\"key\": \"value\"}),\n        ProcessIntent(elem=elem, next_segments=...),\n    )\n</code></pre>"},{"location":"api/core/ops/#runtimesetuperror","title":"RuntimeSetupError","text":"<pre><code>class RuntimeSetupError(Exception):\n    pass\n</code></pre> <p>Raised when handler registration fails (e.g., duplicate signature).</p>"},{"location":"api/http/","title":"Overview","text":""},{"location":"api/http/#http-module","title":"HTTP Module","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>The <code>wxpath.http</code> module provides the HTTP client infrastructure.</p>"},{"location":"api/http/#submodules","title":"Submodules","text":"Module Description crawler HTTP crawlers (<code>Crawler</code>, <code>BaseCrawler</code>, <code>PlaywrightCrawler</code>) TODO: stats Crawler statistics TODO: policy Retry, robots, throttling policies"},{"location":"api/http/#quick-import","title":"Quick Import","text":"<pre><code>from wxpath.http.client import Crawler, Request, Response\nfrom wxpath.http.client.cache import get_cache_backend\nfrom wxpath.http.stats import CrawlerStats\n</code></pre>"},{"location":"api/http/#architecture","title":"Architecture","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   WXPathEngine  \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n                             \u2502                              \u2502  \n                             \u25bc                              \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502      \n                    \u2502     Crawler     \u2502                     \u2502      \n                    \u2502  (BaseCrawler)  \u2502                     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n                             \u2502                              \u2502      \n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n        \u2502                    \u2502                    \u2502         \u2502\n        \u25bc                    \u25bc                    \u25bc         \u2502\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n \u2502  Throttler  \u2502     \u2502 RobotsTxt   \u2502     \u2502 RetryPolicy \u2502    \u2502\n \u2502             \u2502     \u2502   Policy    \u2502     \u2502             \u2502    \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n        \u2502                    \u2502                    \u2502         \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n                             \u2502                              \u2502\n                             \u25bc                              \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502  \n                    \u2502 aiohttp Session \u2502                     \u2502\n                    \u2502   (+ cache)     \u2502 \u2500\u2500\u2500\u2500&gt; Response &gt;\u2500\u2500\u2500\u2500\u2518\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"api/http/#crawler-types","title":"Crawler Types","text":""},{"location":"api/http/#crawler-aiohttp","title":"Crawler (aiohttp)","text":"<p>Standard HTTP crawler using aiohttp. Best for most use cases.</p> <pre><code>from wxpath.http.client import Crawler\n\ncrawler = Crawler(\n    concurrency=16,\n    per_host=4,\n    respect_robots=True\n)\n</code></pre> <p>MORE TO COME!</p>"},{"location":"api/http/#requestresponse-flow","title":"Request/Response Flow","text":"<ol> <li>Engine submits <code>Request</code> to Crawler</li> <li>Crawler checks robots.txt policy (if enabled)</li> <li>Throttler delays request if needed (if enabled)</li> <li>Request sent via aiohttp session</li> <li>Response cached (if enabled)</li> <li><code>Response</code> returned to engine</li> </ol>"},{"location":"api/http/#concurrency-control","title":"Concurrency Control","text":"<p>Two-level semaphore system: - Global semaphore: Limits total concurrent requests - Per-host semaphores: Limits concurrent requests per domain</p> <pre><code>Crawler(\n    concurrency=16,    # Global limit\n    per_host=4         # Per-host limit\n)\n</code></pre>"},{"location":"api/http/crawler/","title":"Crawler","text":""},{"location":"api/http/crawler/#crawler","title":"Crawler","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>HTTP crawler classes for concurrent fetching.</p>"},{"location":"api/http/crawler/#location","title":"Location","text":"<pre><code>from wxpath.http.client import Crawler\nfrom wxpath.http.client.crawler import (\n    BaseCrawler,\n    Crawler,\n    PlaywrightCrawler,\n    FlareSolverrCrawler,\n    get_async_session\n)\n</code></pre>"},{"location":"api/http/crawler/#crawler_1","title":"Crawler","text":"<p>Standard aiohttp-based HTTP crawler.</p> <pre><code>class Crawler:\n    def __init__(\n        self,\n        concurrency: int = None,\n        per_host: int = None,\n        timeout: int = None,\n        *,\n        headers: dict | None = None,\n        proxies: dict | None = None,\n        retry_policy: RetryPolicy | None = None,\n        throttler: AbstractThrottler | None = None,\n        auto_throttle_target_concurrency: float = None,\n        auto_throttle_start_delay: float = None,\n        auto_throttle_max_delay: float = None,\n        respect_robots: bool = True,\n    )\n</code></pre>"},{"location":"api/http/crawler/#parameters","title":"Parameters","text":"Name Type Default Description <code>concurrency</code> int None Global concurrent requests <code>per_host</code> int None Per-host concurrent requests <code>timeout</code> int None Request timeout in seconds <code>headers</code> dict None Default HTTP headers <code>proxies</code> dict None Per-host proxy mapping <code>throttler</code> AbstractThrottler None Request throttler <code>auto_throttle_target_concurrency</code> float None Auto-throttle target <code>auto_throttle_start_delay</code> float None Initial throttle delay <code>auto_throttle_max_delay</code> float None Maximum throttle delay <code>respect_robots</code> bool True Honor robots.txt <code>retry_policy</code> RetryPolicy None Retry configuration"},{"location":"api/http/crawler/#context-manager","title":"Context Manager","text":"<pre><code>from wxpath.http.client import Crawler, Request\n\nasync with Crawler() as crawler:\n    crawler.submit(Request(\"https://example.com\"))\n    async for response in crawler:\n        print(response.status)\n        print(response.body)\n</code></pre>"},{"location":"api/http/crawler/#methods","title":"Methods","text":""},{"location":"api/http/crawler/#submit","title":"submit","text":"<pre><code>def submit(self, req: Request) -&gt; None\n</code></pre> <p>Queue a request for fetching.</p> <p>Parameters: - <code>req</code> - Request to queue</p> <p>Raises: <code>RuntimeError</code> if crawler is closed</p>"},{"location":"api/http/crawler/#build_session","title":"build_session","text":"<pre><code>def build_session(self) -&gt; aiohttp.ClientSession\n</code></pre> <p>Construct an aiohttp session with tracing and connection pooling.</p>"},{"location":"api/http/crawler/#example","title":"Example","text":"<pre><code>from wxpath.http.client import Crawler, Request\n\nasync def main():\n    crawler = Crawler(\n        concurrency=8,\n        per_host=2,\n        headers={'User-Agent': 'my-bot/1.0'},\n        respect_robots=True\n    )\n\n    urls = [\n        \"https://example.com/page1\",\n        \"https://example.com/page2\",\n        \"https://example.com/page3\"\n    ]\n\n    responses = []\n    async with crawler:\n        for url in urls:\n            crawler.submit(Request(url))\n\n        async for resp in crawler:\n            responses.append(resp)\n            if len(responses) &gt;= len(urls):\n                break\n\n    return responses\n</code></pre>"},{"location":"api/integrations/langchain/","title":"LangChain","text":""},{"location":"api/integrations/langchain/#langchain-integration","title":"LangChain Integration","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>The <code>WXPathLoader</code> provides seamless integration between wxpath and LangChain, allowing you to use wxpath expressions as document loaders in LangChain RAG (Retrieval-Augmented Generation) pipelines.</p>"},{"location":"api/integrations/langchain/#location","title":"Location","text":"<pre><code>from wxpath.integrations.langchain.loader import WXPathLoader\n</code></pre>"},{"location":"api/integrations/langchain/#wxpathloader","title":"WXPathLoader","text":"<p>A LangChain document loader that executes wxpath expressions and converts results into LangChain <code>Document</code> objects.</p> <pre><code>class WXPathLoader(BaseLoader):\n    def __init__(self, expression: str, max_depth: int = 1)\n</code></pre>"},{"location":"api/integrations/langchain/#parameters","title":"Parameters","text":"Name Type Default Description <code>expression</code> str - wxpath expression to execute <code>max_depth</code> int 1 Maximum crawl depth for the wxpath query"},{"location":"api/integrations/langchain/#methods","title":"Methods","text":""},{"location":"api/integrations/langchain/#_prep_doc","title":"_prep_doc","text":"<pre><code>def _prep_doc(self, item: (XPathMap | dict)) -&gt; Document\n</code></pre> <p>Prepare a document from a wxpath result.</p> <p>Parameters: - <code>item</code>: The wxpath result (either an XPathMap or dict). Expected to be a dict or XPathMap with a <code>\"text\"</code> key for the document content. Any additional keys become metadata.</p> <p>Best practice is to subclass the loader and override the _prep_doc method. For example:</p> <pre><code>class MyWXPathLoader(WXPathLoader):\n    def _prep_doc(self, item: (XPathMap | dict)) -&gt; Document:\n        # Custom processing here\n        return super()._prep_doc(item)\n</code></pre>"},{"location":"api/integrations/langchain/#lazy_load","title":"lazy_load","text":"<pre><code>def lazy_load(self) -&gt; Iterator[Document]\n</code></pre> <p>Lazy load documents from the wxpath query. Each item yielded by wxpath becomes a LangChain <code>Document</code>.</p> <p>Yields: <code>Document</code> objects with: - <code>page_content</code>: The text content extracted from the wxpath result (from the <code>\"text\"</code> key if present, otherwise string representation) - <code>metadata</code>: Remaining keys from the wxpath result (e.g., <code>url</code>, <code>title</code>, etc.)</p>"},{"location":"api/integrations/langchain/#alazy_load","title":"alazy_load","text":"<pre><code>async def alazy_load(self) -&gt; AsyncIterator[Document]\n</code></pre> <p>Asynchronously lazy load documents from the wxpath query.</p> <p>Yields: <code>Document</code> objects (same structure as <code>lazy_load</code>)</p>"},{"location":"api/integrations/langchain/#load","title":"load","text":"<pre><code>def load(self) -&gt; List[Document]\n</code></pre> <p>Load all documents from the wxpath query into memory.</p> <p>Returns: List of <code>Document</code> objects</p>"},{"location":"api/integrations/langchain/#document-structure","title":"Document Structure","text":"<p>The loader expects wxpath results to be maps (dictionaries) with a <code>\"text\"</code> key for the document content. Any additional keys become metadata.</p> <p>Example wxpath expression structure:</p> <pre><code>expression = \"\"\"\nurl('https://example.com')\n  /map{\n    'text': string-join(//div[@class='content']//text()),\n    'url': string(base-uri(.)),\n    'title': //h1/text()\n  }\n\"\"\"\n</code></pre> <p>This produces <code>Document</code> objects with: - <code>page_content</code>: The joined text from the content div - <code>metadata</code>: <code>{\"url\": \"...\", \"title\": \"...\"}</code></p> <p>If no <code>\"text\"</code> key is present, the entire map is converted to a string for <code>page_content</code>.</p>"},{"location":"api/integrations/langchain/#examples","title":"Examples","text":""},{"location":"api/integrations/langchain/#basic-rag-pipeline","title":"Basic RAG Pipeline","text":"<p>A simple RAG example using wxpath to crawl documentation and create a vector store:</p> <p>See: <code>basic_rag.py</code></p> <p>This example demonstrates: - Loading documents from Python argparse documentation - Splitting documents into chunks - Creating a vector store with embeddings - Building a RAG chain for question answering</p>"},{"location":"api/integrations/langchain/#rolling-window-rag","title":"Rolling Window RAG","text":"<p>An advanced example showing continuous crawling with a rolling buffer:</p> <p>See: <code>rolling_window_rag.py</code></p> <p>This example demonstrates: - Background crawling that continuously updates a document buffer - Thread-safe document management with automatic eviction - Real-time RAG queries against the latest crawled content - Long-running crawler integration with LangChain</p>"},{"location":"api/integrations/langchain/#usage-example","title":"Usage Example","text":"<pre><code>from wxpath.integrations.langchain.loader import WXPathLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_chroma import Chroma\nfrom langchain_ollama import OllamaEmbeddings\n\n# Load documents using wxpath\nloader = WXPathLoader(\n    expression=\"\"\"\n    url('https://docs.python.org/3/library/')\n      ///url(//a/@href[contains(., '/library/')])\n      /map{\n        'text': string-join(//div[@role='main']//text()),\n        'source': string(base-uri(.))\n      }\n    \"\"\",\n    max_depth=2\n)\n\ndocs = loader.load()\n\n# Split and embed\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\n\nvectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=OllamaEmbeddings(model=\"nomic-embed-text\")\n)\n\n# Use with retriever\nretriever = vectorstore.as_retriever()\n</code></pre>"},{"location":"api/integrations/langchain/#async-usage","title":"Async Usage","text":"<p>For async workflows:</p> <pre><code>import asyncio\nfrom wxpath.integrations.langchain.loader import WXPathLoader\n\nasync def main():\n    loader = WXPathLoader(\n        expression=\"url('https://example.com')//div[@class='article']/map{'text': string(.), 'url': string(base-uri(.))}\",\n        max_depth=1\n    )\n\n    docs = []\n    async for doc in loader.alazy_load():\n        docs.append(doc)\n        if len(docs) &gt;= 100:\n            break\n\n    return docs\n\ndocs = asyncio.run(main())\n</code></pre> <p>END: Documentation generated by LLM - requires human review</p>"},{"location":"guide/cli/","title":"CLI","text":""},{"location":"guide/cli/#command-line-interface","title":"Command-Line Interface","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>wxpath provides a CLI for executing expressions directly from the terminal.</p>"},{"location":"guide/cli/#basic-usage","title":"Basic Usage","text":"<pre><code>wxpath \"url('https://quotes.toscrape.com')//a/@href\"\n</code></pre> <p>Output is streamed as newline-delimited JSON (NDJSON).</p>"},{"location":"guide/cli/#options","title":"Options","text":"Option Description Default <code>--depth &lt;n&gt;</code> Maximum crawl depth 1 <code>--verbose</code> Enable verbose output false <code>--debug</code> Enable debug logging false <code>--concurrency &lt;n&gt;</code> Global concurrent fetches 16 <code>--concurrency-per-host &lt;n&gt;</code> Per-host concurrent fetches 8 <code>--header \"Key:Value\"</code> Add custom header (repeatable) - <code>--respect-robots</code> Respect robots.txt true <code>--cache</code> Enable response caching false"},{"location":"guide/cli/#examples","title":"Examples","text":""},{"location":"guide/cli/#simple-link-extraction","title":"Simple Link Extraction","text":"<pre><code>wxpath \"url('https://quotes.toscrape.com')//a/@href\"\n</code></pre>"},{"location":"guide/cli/#deep-crawl-with-custom-headers","title":"Deep Crawl with Custom Headers","text":"<pre><code>wxpath --depth 1 \\\n    --header \"User-Agent: my-bot/1.0 (contact: me@example.com)\" \\\n    \"url('https://quotes.toscrape.com')///url(//a/@href)//title/text()\"\n</code></pre>"},{"location":"guide/cli/#structured-data-extraction","title":"Structured Data Extraction","text":"<pre><code>wxpath --depth 1 \\\n    --header \"User-Agent: my-app/0.1 (contact: you@example.com)\" \\\n    \"url('https://en.wikipedia.org/wiki/Python_(programming_language)') \\\n    /map{ \\\n        'title': (//h1//text())[1] ! normalize-space(.), \\\n        'url': string(base-uri(.)) \\\n    }\"\n</code></pre>"},{"location":"guide/cli/#wikipedia-crawl-with-filters","title":"Wikipedia Crawl with Filters","text":"<pre><code>wxpath --depth 1 \\\n    --header \"User-Agent: my-app/0.1 (contact: you@example.com)\" \\\n    \"url('https://en.wikipedia.org/wiki/Expression_language') \\\n    ///url(//div[@id='mw-content-text']//a/@href[starts-with(., '/wiki/') \\\n        and not(matches(@href, '^(?:/wiki/)?(?:Wikipedia|File|Template|Special|Template_talk|Help):'))]) \\\n    /map{ \\\n        'title':(//span[contains(@class, 'mw-page-title-main')]/text())[1], \\\n        'short_description':(//div[contains(@class, 'shortdescription')]/text())[1], \\\n        'url':string(base-uri(.)), \\\n        'backlink':wx:backlink(.), \\\n        'depth':wx:depth(.) \\\n    }\"\n</code></pre>"},{"location":"guide/cli/#cached-crawl","title":"Cached Crawl","text":"<pre><code># Requires wxpath[cache-sqlite] or wxpath[cache-redis]\nwxpath --cache --depth 1 \\\n    \"url('https://example.com')///url(//a/@href)//title/text()\"\n</code></pre>"},{"location":"guide/cli/#output-format","title":"Output Format","text":"<p>The CLI outputs NDJSON (newline-delimited JSON):</p> <pre><code>{\"title\": \"Page 1\", \"url\": \"https://example.com/page1\"}\n{\"title\": \"Page 2\", \"url\": \"https://example.com/page2\"}\n{\"title\": \"Page 3\", \"url\": \"https://example.com/page3\"}\n</code></pre> <p>This format is easy to pipe to other tools:</p> <pre><code># Count results\nwxpath \"...\" | wc -l\n\n# Filter with jq\nwxpath \"...\" | jq 'select(.title != null)'\n\n# Save to file\nwxpath \"...\" &gt; results.jsonl\n</code></pre>"},{"location":"guide/cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>WXPATH_OUT</code> Output file path for JSONLWriter hook"},{"location":"guide/configuration/","title":"Configuration","text":""},{"location":"guide/configuration/#configuration","title":"Configuration","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>wxpath provides hierarchical configuration through the <code>SETTINGS</code> object.</p>"},{"location":"guide/configuration/#settings-structure","title":"Settings Structure","text":"<pre><code>from wxpath.settings import SETTINGS, CRAWLER_SETTINGS, CACHE_SETTINGS\n</code></pre>"},{"location":"guide/configuration/#crawler-settings","title":"Crawler Settings","text":"<p>Access via <code>CRAWLER_SETTINGS</code> or <code>SETTINGS.http.client.crawler</code>:</p> Setting Type Default Description <code>concurrency</code> int 16 Global concurrent requests <code>per_host</code> int 8 Per-host concurrent requests <code>timeout</code> int 15 Request timeout in seconds <code>headers</code> dict <code>{...}</code> Default HTTP headers <code>proxies</code> dict <code>None</code> Per-host proxy mapping <code>respect_robots</code> bool <code>True</code> Honor robots.txt <code>auto_throttle_target_concurrency</code> float <code>None</code> Target concurrent requests for throttler <code>auto_throttle_start_delay</code> float 0.25 Initial throttle delay <code>auto_throttle_max_delay</code> float 10.0 Maximum throttle delay"},{"location":"guide/configuration/#cache-settings","title":"Cache Settings","text":"<p>Access via <code>CACHE_SETTINGS</code> or <code>SETTINGS.http.client.cache</code>:</p> Setting Type Default Description <code>enabled</code> bool <code>False</code> Enable response caching <code>expire_after</code> timedelta <code>timedelta(days=7)</code> Cache TTL in seconds <code>allowed_methods</code> tuple (\"GET\", \"HEAD\") HTTP methods to cache <code>allowed_codes</code> tuple (200, 203, 301, 302, 307, 308) Status codes to cache <code>ignored_params</code> list [\"utm_*\", \"fbclid\"] Query params to ignore in cache key <code>backend</code> str \"sqlite\" Cache backend (\"sqlite\" or \"redis\") <code>sqlite</code> dict <code>{...}</code> SQLite backend settings <code>redis</code> dict <code>{...}</code> Redis backend settings <p>For SQLite backend:</p> Setting Type Default Description <code>cache_name</code> str \"cache.db\" SQLite cache name <p>For Redis backend:</p> Setting Type Default Description <code>redis.address</code> str \"redis://localhost:6379/0\" Redis connection URL <code>cache_name</code> str \"wxpath:\" Redis cache name"},{"location":"guide/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"guide/configuration/#setting-headers","title":"Setting Headers","text":"<pre><code>from wxpath.settings import CRAWLER_SETTINGS\n\nCRAWLER_SETTINGS.headers = {\n    'User-Agent': 'my-crawler/1.0 (contact: you@example.com)',\n    'Accept-Language': 'en-US,en;q=0.9'\n}\n</code></pre>"},{"location":"guide/configuration/#enabling-caching","title":"Enabling Caching","text":"<pre><code>from wxpath.settings import CACHE_SETTINGS\n\n# SQLite backend (default)\nCACHE_SETTINGS.enabled = True\n\n# Redis backend\nCACHE_SETTINGS.enabled = True\nCACHE_SETTINGS.backend = \"redis\"\nCACHE_SETTINGS.redis.address = \"redis://localhost:6379/0\"\n</code></pre>"},{"location":"guide/configuration/#custom-concurrency","title":"Custom Concurrency","text":"<pre><code>from wxpath.settings import CRAWLER_SETTINGS\n\nCRAWLER_SETTINGS.concurrency = 32\nCRAWLER_SETTINGS.per_host = 4\n</code></pre>"},{"location":"guide/configuration/#proxy-configuration","title":"Proxy Configuration","text":"<pre><code>from wxpath.settings import CRAWLER_SETTINGS\nfrom collections import defaultdict\n\n# Per-host proxies\nCRAWLER_SETTINGS.proxies = {\n    'example.com': 'http://proxy1:8080',\n    'api.example.com': 'http://proxy2:8080'\n}\n\n# Default proxy for all hosts\nCRAWLER_SETTINGS.proxies = defaultdict(lambda: 'http://default-proxy:8080')\n</code></pre>"},{"location":"guide/configuration/#engine-configuration","title":"Engine Configuration","text":"<p>For fine-grained control, configure the engine and crawler directly:</p> <pre><code>from wxpath import wxpath_async_blocking_iter\nfrom wxpath.core.runtime import WXPathEngine\nfrom wxpath.http.client import Crawler\nfrom wxpath.http.policy.retry import RetryPolicy\nfrom wxpath.http.policy.throttler import AutoThrottler\nfrom wxpath.settings import CRAWLER_SETTINGS\n\nCRAWLER_SETTINGS.headers = {'User-Agent': 'my-app/0.4.0 (contact: you@example.com)'}\n\n# Custom retry policy\nretry_policy = RetryPolicy(\n    max_retries=3,\n    retry_statuses={500, 502, 503, 504}\n)\n\n# Custom throttler\nthrottler = AutoThrottler(\n    target_concurrency=2.0,\n    start_delay=1.0,\n    max_delay=30.0\n)\n\n# Create crawler\ncrawler = Crawler(\n    concurrency=8,\n    per_host=2,\n    timeout=15,\n    headers={'User-Agent': 'my-app/1.0'},\n    retry_policy=retry_policy,\n    throttler=throttler,\n    respect_robots=True\n)\n\n# Create engine\nengine = WXPathEngine(\n    crawler=crawler,\n    allowed_response_codes={200, 301, 302},\n    allow_redirects=True\n)\n\npath_expr = \"\"\"\nurl('https://quotes.toscrape.com/tag/humor/', follow=//li[@class='next']/a/@href)\n  //div[@class='quote']\n    /map{\n      'author': (./span/small/text())[1],\n      'text': (./span[@class='text']/text())[1]\n      }\n\"\"\"\n\n# Use engine\nfor item in wxpath_async_blocking_iter(path_expr, max_depth=1, engine=engine):\n    print(item)\n</code></pre>"},{"location":"guide/configuration/#attrdict","title":"AttrDict","text":"<p>Settings use <code>AttrDict</code> for dot-notation access:</p> <pre><code>from wxpath.settings import SETTINGS\n\n# Both work\nSETTINGS['http']['client']['crawler']['concurrency']\nSETTINGS.http.client.crawler.concurrency\n</code></pre>"},{"location":"guide/hooks/","title":"Hooks","text":""},{"location":"guide/hooks/#hooks-experimental","title":"Hooks (Experimental)","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>wxpath provides a pluggable hook system for customizing crawl and extraction behavior.</p>"},{"location":"guide/hooks/#hook-lifecycle","title":"Hook Lifecycle","text":"<p>Hooks can intercept at three points:</p> <ol> <li>post_fetch - After HTTP response, before parsing</li> <li>post_parse - After HTML parsing, before extraction</li> <li>post_extract - After value extraction, before yielding</li> </ol>"},{"location":"guide/hooks/#creating-hooks","title":"Creating Hooks","text":""},{"location":"guide/hooks/#basic-hook","title":"Basic Hook","text":"<pre><code>from wxpath import hooks\n\n@hooks.register\nclass MyHook:\n    def post_fetch(self, ctx, html_bytes):\n        \"\"\"Transform or filter response body.\"\"\"\n        # Return bytes to continue, None to drop\n        return html_bytes\n\n    def post_parse(self, ctx, elem):\n        \"\"\"Transform or filter parsed element.\"\"\"\n        # Return element to continue, None to drop\n        return elem\n\n    def post_extract(self, value):\n        \"\"\"Transform or filter extracted value.\"\"\"\n        # Return value to yield, None to drop\n        return value\n</code></pre>"},{"location":"guide/hooks/#async-hooks","title":"Async Hooks","text":"<pre><code>from wxpath import hooks\n\n@hooks.register\nclass AsyncHook:\n    async def post_fetch(self, ctx, html_bytes):\n        # Async operations supported\n        return html_bytes\n\n    async def post_parse(self, ctx, elem):\n        return elem\n\n    async def post_extract(self, value):\n        return value\n</code></pre> <p>Note: All hooks in a project should use the same style (sync or async). Mixing is not supported.</p>"},{"location":"guide/hooks/#fetchcontext","title":"FetchContext","text":"<p>Hook methods receive a <code>FetchContext</code> with crawl metadata:</p> <pre><code>from wxpath.hooks.registry import FetchContext\n\n# FetchContext fields:\n# - url: str          - Current URL being processed\n# - backlink: str     - URL that linked to this page\n# - depth: int        - Current crawl depth\n# - segments: list    - Remaining expression segments\n# - user_data: dict   - Custom data storage\n</code></pre>"},{"location":"guide/hooks/#example-hooks","title":"Example Hooks","text":""},{"location":"guide/hooks/#language-filter","title":"Language Filter","text":"<pre><code>@hooks.register\nclass OnlyEnglish:\n    def post_parse(self, ctx, elem):\n        lang = elem.xpath('string(/html/@lang)').lower()[:2]\n        return elem if lang in (\"en\", \"\") else None\n</code></pre>"},{"location":"guide/hooks/#url-logger","title":"URL Logger","text":"<pre><code>@hooks.register\nclass URLLogger:\n    def post_fetch(self, ctx, html_bytes):\n        print(f\"Fetched: {ctx.url} (from {ctx.backlink})\")\n        return html_bytes\n</code></pre>"},{"location":"guide/hooks/#value-transformer","title":"Value Transformer","text":"<pre><code>@hooks.register\nclass CleanText:\n    def post_extract(self, value):\n        if isinstance(value, str):\n            return value.strip()\n        return value\n</code></pre>"},{"location":"guide/hooks/#content-filter","title":"Content Filter","text":"<pre><code>@hooks.register\nclass SkipErrors:\n    def post_fetch(self, ctx, html_bytes):\n        if b'error' in html_bytes.lower():\n            return None  # Drop this response\n        return html_bytes\n</code></pre>"},{"location":"guide/hooks/#built-in-hooks","title":"Built-in Hooks","text":""},{"location":"guide/hooks/#serializexpathmapandnodehook","title":"SerializeXPathMapAndNodeHook","text":"<p>Converts <code>XPathMap</code> and <code>XPathNode</code> objects to plain Python types:</p> <pre><code>from wxpath.hooks import SerializeXPathMapAndNodeHook\nfrom wxpath import hooks\n\nhooks.register(SerializeXPathMapAndNodeHook)\n</code></pre>"},{"location":"guide/hooks/#jsonlwriter-ndjsonwriter","title":"JSONLWriter / NDJSONWriter","text":"<p>Writes extracted data to a newline-delimited JSON file:</p> <pre><code>from wxpath import hooks\n\nhooks.register(hooks.JSONLWriter)\n</code></pre> <p>Output file can be configured via <code>WXPATH_OUT</code> environment variable.</p> <p>Features: - Non-blocking writes via background thread - Queue-based buffering - Automatic JSON serialization</p>"},{"location":"guide/hooks/#hook-registration","title":"Hook Registration","text":""},{"location":"guide/hooks/#decorator-style","title":"Decorator Style","text":"<pre><code>@hooks.register\nclass MyHook:\n    pass\n</code></pre>"},{"location":"guide/hooks/#instance-registration","title":"Instance Registration","text":"<pre><code>hooks.register(MyHook())\n</code></pre>"},{"location":"guide/hooks/#class-registration","title":"Class Registration","text":"<pre><code>hooks.register(MyHook)  # Instantiated automatically\n</code></pre>"},{"location":"guide/hooks/#accessing-registered-hooks","title":"Accessing Registered Hooks","text":"<pre><code>from wxpath.hooks.registry import get_hooks\n\nfor hook in get_hooks():\n    print(type(hook).__name__)\n</code></pre>"},{"location":"guide/hooks/#execution-order","title":"Execution Order","text":"<p>Hooks execute in registration order. Each hook can: - Transform the value and pass to next hook - Drop the value by returning <code>None</code> (stops pipeline)</p> <pre><code># First registered, first executed\n@hooks.register\nclass Hook1:\n    def post_extract(self, value):\n        return value.upper()\n\n@hooks.register\nclass Hook2:\n    def post_extract(self, value):\n        return value.strip()\n\n# Result: value.upper().strip()\n</code></pre>"},{"location":"guide/language-design/","title":"Language Design","text":""},{"location":"guide/language-design/#language-design","title":"Language Design","text":"<p>Warning: pre-1.0.0 - APIs and contracts may change.</p> <p>wxpath extends XPath with URL-fetching operators, enabling declarative web traversal expressed directly in path syntax.</p>"},{"location":"guide/language-design/#core-operators","title":"Core Operators","text":""},{"location":"guide/language-design/#urlliteral","title":"<code>url(literal)</code>","text":"<p>Fetches content from a literal URL string:</p> <pre><code>\"url('https://example.com')\"\n</code></pre> <p>Returns an <code>lxml.html.HtmlElement</code> representing the parsed HTML document.</p>"},{"location":"guide/language-design/#urlxpath","title":"<code>url(xpath)</code>","text":"<p>Fetches content from URLs extracted by an XPath expression:</p> <pre><code>\"url('https://example.com')//url(//a/@href)\"\n</code></pre> <p>The inner XPath (<code>//a/@href</code>) extracts URLs from the current document, which are then fetched.</p>"},{"location":"guide/language-design/#urlxpath-deep-crawl-starting-from-descendants","title":"<code>///url(xpath)</code> - Deep Crawl (starting from descendants)","text":"<p>Indicates recursive<sup>*</sup> crawling. The engine follows links extracted by the XPath expression up to <code>max_depth</code>:</p> <p><sup>*</sup> The term \"recursive\" is used informally in this document, however, I do not mean DFS-style recursion. <code>wxpath</code> uses a FIFO queue, and we expand that queue by visiting the URLs present in the current document.</p> <pre><code>\"\"\"\nurl('https://example.com')\n  ///url(//a/@href)\n    //title/text()\n\"\"\"\n</code></pre>"},{"location":"guide/language-design/#url-follow-deep-crawl-starting-from-root","title":"<code>url('...', follow=...)</code> - Deep Crawl (starting from root)","text":"<p>The <code>follow</code> parameter allows you to specify a follow path for recursive crawling at the root node. While this may seem redundant and duplicated behavior found with the <code>///url</code> syntax, it is not. The <code>follow</code> parameter allows you to initiate, yes, a recursive crawl, however it also allows you to begin extracting data from the root node. This is useful in cases where you want to paginate through search pages.</p> <pre><code>path_expr = \"\"\"\nurl('https://quotes.toscrape.com/tag/humor/', follow=//li[@class='next']/a/@href)\n  //div[@class='quote']\n    /map{\n      'author': (./span/small/text())[1],\n      'text': (./span[@class='text']/text())[1]\n      }\n\"\"\"\n</code></pre>"},{"location":"guide/language-design/#url-follow-depth-deep-crawl-starting-from-root-with-depth-limit","title":"<code>url('...', follow=..., depth=...)</code> - Deep Crawl (starting from root) with depth limit","text":"<p>The <code>depth</code> parameter allows you to specify the maximum depth of the crawl. This is useful when you want to limit the depth of the crawl to a specific number of levels.</p>"},{"location":"guide/language-design/#expression-structure","title":"Expression Structure","text":"<p>A wxpath expression is a sequence of segments:</p> <pre><code>url(start) [///url(follow)] [extraction]\n</code></pre> <p>Where: - <code>url(start)</code> - Initial URL(s) to fetch - <code>///url(follow)</code> - Optional recursive crawl pattern - <code>extraction</code> - XPath to extract data from fetched pages</p>"},{"location":"guide/language-design/#segment-types","title":"Segment Types","text":"Segment Description <code>url('...')</code> Fetch from literal URL <code>/url(xpath)</code> Fetch from XPath-extracted URLs <code>//url(xpath)</code> One-hop link following <code>///url(xpath)</code> Recursive deep crawl XPath Standard XPath 3.1 expression"},{"location":"guide/language-design/#xpath-31-features","title":"XPath 3.1 Features","text":"<p>wxpath uses <code>elementpath</code> for XPath 3.1 support, enabling:</p>"},{"location":"guide/language-design/#maps","title":"Maps","text":"<pre><code>\"/map{ 'key1': //xpath1, 'key2': //xpath2 }\"\n</code></pre>"},{"location":"guide/language-design/#arrays","title":"Arrays","text":"<pre><code>\"/array{ //item1, //item2 }\"\n</code></pre>"},{"location":"guide/language-design/#built-in-functions","title":"Built-in Functions","text":"<p>Standard XPath 3.1 functions plus wxpath-specific:</p> Function Description <code>base-uri(.)</code> Current document URL <code>wx:current-url()</code> Current document URL <code>wx:backlink(.)</code> URL that linked to current page <code>wx:depth(.)</code> Current crawl depth <code>wx:fetch-time()</code> Fetch time <code>wx:elapsed()</code> Fetch time <code>wx:status-code()</code> Current HTTP status code <code>wx:elem()</code> Current HTML element (Warning: memory will explode with deep crawls) <code>wx:internal-links()</code> Internal links <code>wx:external-links()</code> External links <code>wx:main-article-text()</code> Main article text (reimplimentation of eatiht)"},{"location":"guide/language-design/#future-potential-functions","title":"Future Potential Functions","text":"Function Description <code>post(...)</code> HTTP POST <code>wx:limit()</code> Cap results at expression level <code>wx:clean-text()</code> Normalizes whitespace and newlines <code>wx:status-text()</code> Current HTTP status text <code>wx:delay()</code> Sleep for the specified number of seconds <code>wx:headers()</code> HTTP headers"},{"location":"guide/language-design/#examples","title":"Examples","text":""},{"location":"guide/language-design/#extract-links-with-context","title":"Extract Links with Context","text":"<pre><code>\"\"\"\nurl('https://example.com')\n  //a/map{\n    'text': string(.),\n    'href': @href,\n    'source': string(base-uri(.))\n  }\n\"\"\"\n</code></pre>"},{"location":"guide/language-design/#filtered-deep-crawl","title":"Filtered Deep Crawl","text":"<pre><code>\"\"\"\nurl('https://en.wikipedia.org/wiki/Main_Page')\n  ///url(\n    //a/@href[\n      starts-with(., '/wiki/')\n      and not(contains(., ':'))\n    ]\n  )\n  /map{\n    'title': //h1/span/text() ! normalize-space(.),\n    'url': string(base-uri(.))\n  }\n\"\"\"\n</code></pre>"},{"location":"guide/language-design/#pagination-pattern","title":"Pagination Pattern","text":"<pre><code>\"\"\"\nurl('https://example.com/page/1')\n  ///url(//a[@class='next']/@href)\n    //div[@class='item']/text()\n\"\"\"\n</code></pre>"},{"location":"guide/language-design/#execution-model","title":"Execution Model","text":"<ol> <li>Parse - Expression is parsed into an AST of segments</li> <li>Seed - Initial <code>url('...')</code> creates the first crawl tasks</li> <li>BFS-ish Traversal - Tasks are processed breadth-first with deduplication</li> <li>Extraction - XPath segments extract data from fetched documents</li> <li>Streaming - Results are yielded as soon as available</li> </ol> <p>The engine handles: - URL deduplication (best-effort, per-crawl) - Concurrent fetching with configurable limits - robots.txt compliance - Adaptive throttling - Retry policies</p>"},{"location":"guide/language-design/#best-practices","title":"Best Practices","text":"<ol> <li>Use XPath predicates to filter links early and avoid traversal explosion</li> <li>Set appropriate <code>max_depth</code> to bound crawl scope</li> <li>Add <code>User-Agent</code> headers for polite crawling</li> <li>Use caching for development and resumable crawls</li> </ol>"},{"location":"tui/","title":"Overview","text":""},{"location":"tui/#wxpath-tui-interactive-expression-testing","title":"wxpath TUI - Interactive Expression Testing","text":"<p>NOTE: I highly recommended you enable caching (Ctrl+L) for faster execution, and set depth (i.e., <code>url('...', depth=...)</code>) for capped crawls to be polite to the servers you are crawling.</p>"},{"location":"tui/#features","title":"\u2728 Features","text":""},{"location":"tui/#top-panel-expression-editor","title":"\ud83d\udcdd Top Panel - Expression Editor","text":"<ul> <li>Syntax-aware text editing</li> <li>Real-time validation feedback</li> <li>Smart bracket/quote matching</li> <li>Inline error detection</li> </ul>"},{"location":"tui/#bottom-panel-live-output-display","title":"\ud83d\udcca Bottom Panel - Live Output Display","text":"<ul> <li>HTML Elements: Formatted with partial content display (first 300 chars)</li> <li>Dict/XPathMap: Automatically rendered as elegant tables</li> <li>Sortable columns: Click a column header to sort by that column; click again to toggle ascending/descending</li> <li>Export: Export table data to CSV or JSON (Ctrl+E or Export button)</li> <li>Error Messages: Clear validation and execution feedback  </li> <li>Waiting State: Shows when expression is incomplete or invalid</li> <li>Streaming Results: Live updates as data arrives (max 10 items shown)</li> <li>Cancel Crawl: Press Escape during a run to stop the crawl; results already received stay in the table</li> </ul>"},{"location":"tui/#installation","title":"\ud83d\ude80 Installation","text":"<p>Install wxpath with TUI support:</p> <pre><code>pip install -e \".[tui]\"\n</code></pre> <p>Or install textual separately if wxpath is already installed:</p> <pre><code>pip install textual&gt;=1.0.0\n</code></pre>"},{"location":"tui/#usage","title":"\ud83c\udfaf Usage","text":""},{"location":"tui/#launch-the-tui","title":"Launch the TUI","text":"<pre><code># Using the installed command\nwxpath-tui\n\n# Or run as module\npython -m wxpath.tui\n</code></pre>"},{"location":"tui/#keybindings","title":"Keybindings","text":"Key Action Description <code>Ctrl+R</code> or <code>F5</code> Execute Run the current expression <code>Escape</code> Cancel Crawl Stop the running crawl; partial results are kept <code>Ctrl+E</code> Export Export table data (CSV or JSON) <code>Ctrl+C</code> Clear Clear the output panel <code>Ctrl+H</code> Headers Configure HTTP headers (JSON) <code>Ctrl+Shift+S</code> Settings Edit persistent crawler settings (CONCURRENCY, PER_HOST, RESPECT_ROBOTS) <code>Ctrl+L</code> Cache Toggle HTTP caching on/off (SQLite for now) <code>Ctrl+Shift+D</code> Toggle Debug Show or hide the debug panel <code>Ctrl+Q</code> Quit Exit the application Click column header Sort Sort table by that column; click again to toggle ascending/descending"},{"location":"tui/#example-expressions","title":"\ud83d\udcda Example Expressions","text":""},{"location":"tui/#1-simple-text-extraction","title":"1. Simple Text Extraction","text":"<pre><code>url('https://quotes.toscrape.com')//span[@class='text']/text()\n</code></pre> Output: List of text strings"},{"location":"tui/#2-map-extraction-table-view","title":"2. Map Extraction (Table View)","text":"<pre><code>url('https://quotes.toscrape.com')//div[@class='quote']/map {\n  'quote': .//span[@class='text']/text(),\n  'author': .//span[@class='author']/text(),\n  'tags': .//div[@class='tags']//a/text()\n}\n</code></pre> Output: Formatted table with columns: quote, author, tags"},{"location":"tui/#3-link-following-crawling","title":"3. Link Following (Crawling)","text":"<pre><code>url('https://quotes.toscrape.com')\n  ///url(//a[contains(@href, '/author/')]/@href)\n    //h3[@class='author-title']/text()\n</code></pre> Output: Author names from linked pages"},{"location":"tui/#4-html-element-extraction","title":"4. HTML Element Extraction","text":"<pre><code>url('https://quotes.toscrape.com')//div[@class='quote']\n</code></pre> Output: Partial HTML of matching elements"},{"location":"tui/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>The TUI embodies wxpath's architectural philosophy:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Textual Framework                 \u2502  \u2190 Modern TUI with Rich rendering\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Expression Editor (TextArea)      \u2502  \u2190 Real-time validation\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   WXPath Engine                     \u2502  \u2190 Async concurrent execution\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Output Renderer                   \u2502  \u2190 Smart formatting (HTML/Table)\n\u2502   \u2022 HTML Elements                   \u2502\n\u2502   \u2022 Dict \u2192 Table                    \u2502\n\u2502   \u2022 Error Messages                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tui/#key-components","title":"Key Components","text":"<ul> <li>Textual: Modern terminal UI framework with Rich rendering</li> <li>WXPath Engine: Async execution with concurrent crawling</li> <li>Reactive Validation: Live feedback as you type</li> <li>Smart Formatting: Automatic detection and formatting of result types</li> <li>Hook System: XPathMap serialization for clean dict output</li> </ul>"},{"location":"tui/#how-it-works","title":"\ud83d\udd0d How It Works","text":""},{"location":"tui/#expression-validation","title":"Expression Validation","text":"<p>The TUI validates your expression in real-time:</p> <ol> <li>Balance Checking: Parentheses <code>()</code>, brackets <code>[]</code>, braces <code>{}</code></li> <li>Quote Matching: Single <code>'</code> and double <code>\"</code> quotes</li> <li>Syntax Validation: Parser checks for valid wxpath syntax</li> <li>Feedback Display: Shows \"Waiting\" until expression is complete</li> </ol>"},{"location":"tui/#execution-flow","title":"Execution Flow","text":"<pre><code>User Types \u2192 Validation \u2192 Press Execute \u2192 Parse \u2192 Run Engine \u2192 Format \u2192 Display\n     \u2193           \u2193             \u2193            \u2193         \u2193          \u2193         \u2193\n  TextArea   Balance?     Parser OK?   AST Built  HTTP Req  HTML/Table  Output\n              \u2193                          \n          \"Waiting\" or \"Valid\"\n</code></pre>"},{"location":"tui/#output-formatting","title":"Output Formatting","text":"Input Type Output Format Details <code>HtmlElement</code> Partial HTML string First 300 chars, escaped <code>dict</code> (single) Indented key-value Pretty-printed <code>[dict, dict, ...]</code> Table Columns auto-detected <code>str</code> Plain text Truncated if &gt;200 chars Other String repr Generic fallback"},{"location":"tui/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"tui/#persistent-settings-ctrlshifts","title":"Persistent Settings (Ctrl+Shift+S)","text":"<p>Crawler settings are saved to a config file and reused across sessions:</p> Setting Description Default CONCURRENCY Maximum concurrent HTTP requests 16 PER_HOST Maximum concurrent requests per host 8 RESPECT_ROBOTS Whether to respect robots.txt ON (true) <ul> <li>Config file: <code>~/.config/wxpath/tui_settings.json</code> (or <code>$XDG_CONFIG_HOME/wxpath/tui_settings.json</code> if set).</li> <li>When applied: Values are used for the next expression run after you save.</li> <li>Extending: New settings can be added by extending the schema in <code>src/wxpath/tui_settings.py</code> and using the value where the crawler/engine is created.</li> </ul>"},{"location":"tui/quickstart/","title":"Quickstart","text":""},{"location":"tui/quickstart/#wxpath-tui-quick-start-guide","title":"wxpath TUI - Quick Start Guide","text":"<p>Get started with the wxpath TUI in 5 minutes!</p> <p>NOTE: I highly recommended you enable caching (Ctrl+L) for faster execution, and set <code>url('...', depth=...)</code> for capped crawls to be polite to the servers you are crawling.</p>"},{"location":"tui/quickstart/#step-0-preview","title":"Step 0: Preview","text":"Wxpath TUI Demo screenshot"},{"location":"tui/quickstart/#step-1-install-dependencies","title":"Step 1: Install Dependencies","text":"<pre><code># Install textual (required for TUI)\npip install textual\n\n# Or install wxpath with TUI support\npip install -e \".[tui]\"\n</code></pre>"},{"location":"tui/quickstart/#step-2-launch-the-tui","title":"Step 2: Launch the TUI","text":"<pre><code># Using command (if installed)\nwxpath-tui\n\n# Or as module\npython -m wxpath.tui\n</code></pre>"},{"location":"tui/quickstart/#step-3-your-first-expression","title":"Step 3: Your First Expression","text":""},{"location":"tui/quickstart/#in-the-tui","title":"In the TUI:","text":"<ol> <li>Top panel shows an example expression</li> <li>Press Ctrl+R or F5 to execute</li> <li>Bottom panel shows results   a. Debug panel shows debug messages</li> </ol>"},{"location":"tui/quickstart/#try-these-expressions","title":"Try These Expressions:","text":"<p>Example 1: Extract Text </p><pre><code>url('https://quotes.toscrape.com')//span[@class='text']/text()\n</code></pre><p></p> <p>Example 2: Extract as Table </p><pre><code>url('https://quotes.toscrape.com')//div[@class='quote']/map {\n  'quote': .//span[@class='text']/text(),\n  'author': .//span[@class='author']/text()\n}\n</code></pre><p></p> <p>Example 3: Follow Links </p><pre><code>url('https://quotes.toscrape.com')\n  ///url(//a/@href)\n    //h3/text()\n</code></pre><p></p>"},{"location":"tui/quickstart/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"Key Action <code>Ctrl+R</code> or <code>F5</code> Execute expression <code>Escape</code> Cancel crawl (keep partial results) <code>Ctrl+C</code> Clear output <code>Ctrl+Q</code> Quit"},{"location":"tui/quickstart/#understanding-output","title":"Understanding Output","text":""},{"location":"tui/quickstart/#waiting-state","title":"\"Waiting\" State","text":"<p>The TUI shows \"Waiting\" when: - Expression is empty - Missing closing parentheses, brackets, or braces - Unclosed quotes</p>"},{"location":"tui/quickstart/#valid-expression","title":"Valid Expression","text":"<p>Shows green message when expression is complete and ready to execute.</p>"},{"location":"tui/quickstart/#results-display","title":"Results Display","text":"<p>Text Results: </p><pre><code>Result 1:\n  \"The world as we have created it...\"\n\nResult 2:\n  \"It is our choices that show...\"\n</code></pre><p></p> <p>Table Results (from <code>map {...}</code>): </p><pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 quote                           \u2502 author           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \"The world as we have...\"       \u2502 Albert Einstein  \u2502\n\u2502 \"It is our choices...\"          \u2502 J.K. Rowling     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre><p></p> <p>HTML Elements: </p><pre><code>Result 1:\n  &lt;div class=\"quote\"&gt;\n</code></pre><p></p>"},{"location":"tui/quickstart/#common-issues","title":"Common Issues","text":""},{"location":"tui/quickstart/#issue-command-not-found-wxpath-tui","title":"Issue: \"command not found: wxpath-tui\"","text":"<p>Solution: </p><pre><code># Run as module instead\npython -m wxpath.tui\n</code></pre><p></p>"},{"location":"tui/quickstart/#issue-no-module-named-textual","title":"Issue: \"No module named 'textual'\"","text":"<p>Solution: </p><pre><code>pip install textual\n</code></pre><p></p>"},{"location":"tui/quickstart/#issue-timeout-after-30s","title":"Issue: \"Timeout after 30s\"","text":"<p>Solution: - The site may be slow or unresponsive - Try a faster/local site first - Use the demo: <code>python demo_tui.py</code></p>"},{"location":"tui/quickstart/#issue-no-results-returned","title":"Issue: \"No results returned\"","text":"<p>Solution: - Expression is valid but found no matches - Check XPath selectors - Verify URL is accessible - Try simpler expression first</p>"},{"location":"tui/quickstart/#tips-for-success","title":"Tips for Success","text":""},{"location":"tui/quickstart/#1-start-simple","title":"1. Start Simple","text":"<pre><code># Begin with basic extraction\nurl('https://example.com')//h1/text()\n</code></pre>"},{"location":"tui/quickstart/#2-add-complexity-gradually","title":"2. Add Complexity Gradually","text":"<pre><code># Then add map structure\nurl('https://example.com')//article/map {\n  'title': .//h1/text()\n}\n</code></pre>"},{"location":"tui/quickstart/#3-cancel-long-crawls","title":"3. Cancel Long Crawls","text":"<ul> <li>Press Escape to stop a running crawl at any time</li> <li>Partial results stay in the table; a status line shows how many were received</li> </ul>"},{"location":"tui/quickstart/#4-test-incrementally","title":"4. Test Incrementally","text":"<ul> <li>Execute after each change</li> <li>Verify results before adding more</li> <li>Use Ctrl+C to clear between tests</li> </ul>"},{"location":"tui/quickstart/#5-watch-validation","title":"5. Watch Validation","text":"<ul> <li>Green = ready to execute</li> <li>Yellow = incomplete or error</li> <li>Bottom panel shows helpful messages</li> </ul>"},{"location":"tui/quickstart/#next-steps","title":"Next Steps","text":""},{"location":"tui/quickstart/#learn-more","title":"Learn More","text":"<ul> <li>Read index.md for detailed features</li> <li>See examples.md for more expressions</li> </ul>"},{"location":"tui/quickstart/#explore-wxpath","title":"Explore wxpath","text":"<ul> <li>Getting Started</li> <li>Language Design</li> <li>Examples</li> </ul>"},{"location":"tui/quickstart/#get-help","title":"Get Help","text":"<ul> <li>Check wxpath docs</li> <li>Report issues on GitHub</li> <li>Review error messages in TUI</li> <li>Try simpler expressions first</li> </ul>"},{"location":"tui/quickstart/#quick-reference-card","title":"Quick Reference Card","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              wxpath TUI Quick Reference                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 COMMANDS                                                \u2502\n\u2502   wxpath-tui              Launch TUI                    \u2502\n\u2502   python -m wxpath.tui    Alternative launch            \u2502\n\u2502   python demo_tui.py      Offline demo                  \u2502\n\u2502                                                         \u2502\n\u2502 KEYBOARD                                                \u2502\n\u2502   Ctrl+R, F5              Execute expression            \u2502\n\u2502   Escape                  Cancel crawl (keep results)   \u2502\n\u2502   Ctrl+C                  Clear output                  \u2502\n\u2502   Ctrl+Q                  Quit                          \u2502\n\u2502                                                         \u2502\n\u2502 EXPRESSIONS                                             \u2502\n\u2502   url('...')//tag         Extract elements              \u2502\n\u2502   url('...')//tag/text()  Extract text                  \u2502\n\u2502   url('...')//tag/@attr   Extract attributes            \u2502\n\u2502   url('...')//tag/map{..} Extract as dict/table         \u2502\n\u2502   ///url(...)             Follow links (crawl)          \u2502\n\u2502                                                         \u2502\n\u2502 OUTPUT                                                  \u2502\n\u2502   Text \u2192 List of strings                                \u2502\n\u2502   Dict \u2192 Table with columns                             \u2502\n\u2502   HTML \u2192 Partial element (300 chars)                    \u2502\n\u2502   Error \u2192 Clear message                                 \u2502\n\u2502                                                         \u2502\n\u2502 LIMITS                                                  \u2502\n\u2502   Results: 10 items max                                 \u2502\n\u2502   Depth: 1 level max                                    \u2502\n\u2502   Timeout: 30 seconds                                   \u2502\n\u2502   Table: 50 rows max                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tui/quickstart/#success","title":"Success!","text":"<p>You're now ready to use the wxpath TUI for interactive expression testing!</p> <p>Happy crawling! \ud83d\udd77\ufe0f\u2728</p> <p>For detailed documentation, see: - index.md - Full user guide - examples.md - More examples</p>"}]}